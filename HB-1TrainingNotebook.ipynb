{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "893aa560-db02-48ed-81b5-6242d5ad22dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.44.2)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.0.0)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.4.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.24.7)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.19.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (75.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.6.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/anaconda3/lib/python3.12/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers datasets torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56d31a48-bb0e-4a66-9893-63f8df1ecf77",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99d33070-dc58-432b-9355-722fbe598bf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 167278 entries, 0 to 167277\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   CASE_NUMBER                     167278 non-null  object \n",
      " 1   CASE_STATUS                     167278 non-null  object \n",
      " 2   CASE_RECEIVED_DATE              167278 non-null  object \n",
      " 3   DECISION_DATE                   167278 non-null  object \n",
      " 4   EMPLOYER_NAME                   167278 non-null  object \n",
      " 5   PREVAILING_WAGE_SUBMITTED       167278 non-null  float64\n",
      " 6   PREVAILING_WAGE_SUBMITTED_UNIT  167278 non-null  object \n",
      " 7   PAID_WAGE_SUBMITTED             167278 non-null  float64\n",
      " 8   PAID_WAGE_SUBMITTED_UNIT        167278 non-null  object \n",
      " 9   JOB_TITLE                       167278 non-null  object \n",
      " 10  WORK_CITY                       167275 non-null  object \n",
      " 11  EDUCATION_LEVEL_REQUIRED        11063 non-null   object \n",
      " 12  COLLEGE_MAJOR_REQUIRED          11051 non-null   object \n",
      " 13  EXPERIENCE_REQUIRED_Y_N         11093 non-null   object \n",
      " 14  EXPERIENCE_REQUIRED_NUM_MONTHS  4965 non-null    float64\n",
      " 15  COUNTRY_OF_CITIZENSHIP          11093 non-null   object \n",
      " 16  PREVAILING_WAGE_SOC_CODE        167278 non-null  object \n",
      " 17  PREVAILING_WAGE_SOC_TITLE       167278 non-null  object \n",
      " 18  WORK_STATE                      167278 non-null  object \n",
      " 19  WORK_POSTAL_CODE                53674 non-null   object \n",
      " 20  FULL_TIME_POSITION_Y_N          156185 non-null  object \n",
      " 21  VISA_CLASS                      167278 non-null  object \n",
      " 22  PREVAILING_WAGE_PER_YEAR        167210 non-null  float64\n",
      " 23  PAID_WAGE_PER_YEAR              167278 non-null  float64\n",
      " 24  JOB_TITLE_SUBGROUP              167278 non-null  object \n",
      " 25  order                           167278 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(20)\n",
      "memory usage: 33.2+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset using absolute path\n",
    "file_path = \"/Users/audrey/H-1B Insights AI/USAforeignworkerssalarydata-1556559586172.xlsx\"\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Check the first few rows\n",
    "df.head()\n",
    "\n",
    "# Summary of the dataset\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366c0278-eb13-4b18-a982-41af626cd6cc",
   "metadata": {},
   "source": [
    "### Data Format\n",
    "- CASE_NUMBER: Unique identifier for each visa application case.\n",
    "- CASE_STATUS: Status of the case (e.g., certified, denied).\n",
    "- EMPLOYER_NAME: The name of the employer filing the H-1B petition.\n",
    "- PREVAILING_WAGE_SUBMITTED: The wage submitted by the employer as part of the application.\n",
    "- PAID_WAGE_SUBMITTED: The wage paid to the worker.\n",
    "- JOB_TITLE: The job title for which the H-1B visa is filed.\n",
    "- WORK_CITY and WORK_STATE: The city and state where the job is located.\n",
    "- VISA_CLASS: Type of visa being applied for.\n",
    "- PREVAILING_WAGE_PER_YEAR and PAID_WAGE_PER_YEAR: The annualized wages (submitted and paid).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30cc6a9b-2c18-45d5-bef7-44bf407a0240",
   "metadata": {},
   "source": [
    "# Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c795c6ba-5d32-4b08-aedb-1d05780b1b0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 167278 entries, 0 to 167277\n",
      "Data columns (total 26 columns):\n",
      " #   Column                          Non-Null Count   Dtype  \n",
      "---  ------                          --------------   -----  \n",
      " 0   CASE_NUMBER                     167278 non-null  object \n",
      " 1   CASE_STATUS                     167278 non-null  object \n",
      " 2   CASE_RECEIVED_DATE              167278 non-null  object \n",
      " 3   DECISION_DATE                   167278 non-null  object \n",
      " 4   EMPLOYER_NAME                   167278 non-null  object \n",
      " 5   PREVAILING_WAGE_SUBMITTED       167278 non-null  float64\n",
      " 6   PREVAILING_WAGE_SUBMITTED_UNIT  167278 non-null  object \n",
      " 7   PAID_WAGE_SUBMITTED             167278 non-null  float64\n",
      " 8   PAID_WAGE_SUBMITTED_UNIT        167278 non-null  object \n",
      " 9   JOB_TITLE                       167278 non-null  object \n",
      " 10  WORK_CITY                       167275 non-null  object \n",
      " 11  EDUCATION_LEVEL_REQUIRED        11063 non-null   object \n",
      " 12  COLLEGE_MAJOR_REQUIRED          11051 non-null   object \n",
      " 13  EXPERIENCE_REQUIRED_Y_N         11093 non-null   object \n",
      " 14  EXPERIENCE_REQUIRED_NUM_MONTHS  4965 non-null    float64\n",
      " 15  COUNTRY_OF_CITIZENSHIP          11093 non-null   object \n",
      " 16  PREVAILING_WAGE_SOC_CODE        167278 non-null  object \n",
      " 17  PREVAILING_WAGE_SOC_TITLE       167278 non-null  object \n",
      " 18  WORK_STATE                      167278 non-null  object \n",
      " 19  WORK_POSTAL_CODE                53674 non-null   object \n",
      " 20  FULL_TIME_POSITION_Y_N          156185 non-null  object \n",
      " 21  VISA_CLASS                      167278 non-null  object \n",
      " 22  PREVAILING_WAGE_PER_YEAR        167210 non-null  float64\n",
      " 23  PAID_WAGE_PER_YEAR              167278 non-null  float64\n",
      " 24  JOB_TITLE_SUBGROUP              167278 non-null  object \n",
      " 25  order                           167278 non-null  int64  \n",
      "dtypes: float64(5), int64(1), object(20)\n",
      "memory usage: 33.2+ MB\n",
      "None\n",
      "          CASE_NUMBER CASE_STATUS CASE_RECEIVED_DATE DECISION_DATE  \\\n",
      "0  I-200-14073-248840      denied         2014-03-14    2014-03-21   \n",
      "1       A-15061-55212      denied         2015-03-19    2015-03-19   \n",
      "2  I-200-13256-001092      denied         2013-09-13    2013-09-23   \n",
      "3  I-200-14087-353657      denied         2014-03-28    2014-04-07   \n",
      "4  I-203-14259-128844      denied         2014-09-16    2014-09-23   \n",
      "\n",
      "                                       EMPLOYER_NAME  \\\n",
      "0                ADVANCED TECHNOLOGY GROUP USA, INC.   \n",
      "1                     SAN FRANCISCO STATE UNIVERSITY   \n",
      "2                                    CAROUSEL SCHOOL   \n",
      "3  HARLINGEN CONSOLIDATED INDEPENDENT SCHOOL DIST...   \n",
      "4                        SIGNAL SCIENCES CORPORATION   \n",
      "\n",
      "   PREVAILING_WAGE_SUBMITTED PREVAILING_WAGE_SUBMITTED_UNIT  \\\n",
      "0                  6217100.0                           year   \n",
      "1                  5067600.0                           year   \n",
      "2                  4947000.0                           year   \n",
      "3                   251052.0                          month   \n",
      "4                    84573.0                      bi-weekly   \n",
      "\n",
      "   PAID_WAGE_SUBMITTED PAID_WAGE_SUBMITTED_UNIT  \\\n",
      "0              62171.0                     year   \n",
      "1              91440.0                     year   \n",
      "2              49470.0                     year   \n",
      "3              43800.0                     year   \n",
      "4             170000.0                     year   \n",
      "\n",
      "                          JOB_TITLE  ... PREVAILING_WAGE_SOC_CODE  \\\n",
      "0                 SOFTWARE ENGINEER  ...                  15-1132   \n",
      "1  Assistant Professor of Marketing  ...                  25-1011   \n",
      "2         SPECIAL EDUCATION TEACHER  ...                  25-2052   \n",
      "3                   SCIENCE TEACHER  ...                  25-1042   \n",
      "4          SENIOR SOFTWARE ENGINEER  ...                  15-1133   \n",
      "\n",
      "                           PREVAILING_WAGE_SOC_TITLE  WORK_STATE  \\\n",
      "0                  Software Developers, Applications          IL   \n",
      "1                   Business Teachers, Postsecondary  CALIFORNIA   \n",
      "2  Special Education Teachers, Kindergarten and E...          CA   \n",
      "3         Biological Science Teachers, Postsecondary          TX   \n",
      "4              Software Developers, Systems Software          OR   \n",
      "\n",
      "  WORK_POSTAL_CODE  FULL_TIME_POSITION_Y_N      VISA_CLASS  \\\n",
      "0              NaN                       y            H-1B   \n",
      "1          94132.0                     NaN       greencard   \n",
      "2              NaN                       y            H-1B   \n",
      "3              NaN                       y            H-1B   \n",
      "4              NaN                       y  E-3 Australian   \n",
      "\n",
      "  PREVAILING_WAGE_PER_YEAR PAID_WAGE_PER_YEAR   JOB_TITLE_SUBGROUP order  \n",
      "0                      NaN            62171.0    software engineer     1  \n",
      "1                      NaN            91440.0  assistant professor     2  \n",
      "2                      NaN            49470.0              teacher     3  \n",
      "3                      NaN            43800.0              teacher     4  \n",
      "4                      NaN           170000.0    software engineer     5  \n",
      "\n",
      "[5 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "# Create a new DataFrame for cleaned data \n",
    "df_cleaned = df.copy()\n",
    "\n",
    "# Drop rows where key columns have missing data \n",
    "df_cleaned = df_cleaned.dropna(subset=['CASE_NUMBER', 'EMPLOYER_NAME', 'JOB_TITLE'])\n",
    "\n",
    "# Normalize date formats to a consistent format (YYYY-MM-DD)\n",
    "df_cleaned['CASE_RECEIVED_DATE'] = pd.to_datetime(df_cleaned['CASE_RECEIVED_DATE'], errors='coerce').dt.date\n",
    "df_cleaned['DECISION_DATE'] = pd.to_datetime(df_cleaned['DECISION_DATE'], errors='coerce').dt.date\n",
    "\n",
    "# Ensure numeric columns are correctly formatted as floats\n",
    "numeric_columns = ['PREVAILING_WAGE_SUBMITTED', 'PAID_WAGE_SUBMITTED', 'PREVAILING_WAGE_PER_YEAR', 'PAID_WAGE_PER_YEAR']\n",
    "df_cleaned[numeric_columns] = df_cleaned[numeric_columns].apply(pd.to_numeric, errors='coerce')\n",
    "\n",
    "# Save the cleaned data to a new Excel file to preserve the original\n",
    "cleaned_file_path = \"/Users/audrey/H-1B Insights AI/Cleaned_H1B_Visa_Data.xlsx\"\n",
    "df_cleaned.to_excel(cleaned_file_path, index=False)\n",
    "\n",
    "# Check the cleaned data summary and first few rows\n",
    "print(df_cleaned.info())\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc126a5-d719-46d9-9acb-3649928fcfa4",
   "metadata": {},
   "source": [
    "# Convert Rows to Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6d4cec6-d886-4527-802a-7df508c7cfa0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    On 2014-03-14, ADVANCED TECHNOLOGY GROUP USA, ...\n",
       "1    On 2015-03-19, SAN FRANCISCO STATE UNIVERSITY ...\n",
       "2    On 2013-09-13, CAROUSEL SCHOOL applied for a H...\n",
       "3    On 2014-03-28, HARLINGEN CONSOLIDATED INDEPEND...\n",
       "4    On 2014-09-16, SIGNAL SCIENCES CORPORATION app...\n",
       "Name: Sentence, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to convert each row into a sentence\n",
    "def row_to_sentence(row):\n",
    "    return (\n",
    "        f\"On {row['CASE_RECEIVED_DATE']}, {row['EMPLOYER_NAME']} applied for a {row['VISA_CLASS']} visa \"\n",
    "        f\"for the position of {row['JOB_TITLE']} in {row['WORK_STATE']}. \"\n",
    "        f\"The prevailing wage was {row['PREVAILING_WAGE_SUBMITTED']} {row['PREVAILING_WAGE_SUBMITTED_UNIT']} \"\n",
    "        f\"and the paid wage was {row['PAID_WAGE_SUBMITTED']} {row['PAID_WAGE_SUBMITTED_UNIT']}. \"\n",
    "        f\"The application was {row['CASE_STATUS']} on {row['DECISION_DATE']}.\"\n",
    "    )\n",
    "\n",
    "# Apply the function to each row\n",
    "df_cleaned['Sentence'] = df_cleaned.apply(row_to_sentence, axis=1)\n",
    "\n",
    "# Show the first few sentences\n",
    "df_cleaned['Sentence'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1d9f2482-fa49-49c1-aef9-97b9fde8ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                           On 2014-03-14, ADVANCED TECHNOLOGY GROUP USA, INC. applied for a H-1B visa for the position of SOFTWARE ENGINEER in IL. The prevailing wage was 6217100.0 year and the paid wage was 62171.0 year. The application was denied on 2014-03-21.\n",
      "1    On 2015-03-19, SAN FRANCISCO STATE UNIVERSITY applied for a greencard visa for the position of Assistant Professor of Marketing in CALIFORNIA. The prevailing wage was 5067600.0 year and the paid wage was 91440.0 year. The application was denied on 2015-03-19.\n",
      "2                                       On 2013-09-13, CAROUSEL SCHOOL applied for a H-1B visa for the position of SPECIAL EDUCATION TEACHER in CA. The prevailing wage was 4947000.0 year and the paid wage was 49470.0 year. The application was denied on 2013-09-23.\n",
      "3              On 2014-03-28, HARLINGEN CONSOLIDATED INDEPENDENT SCHOOL DISTRICT applied for a H-1B visa for the position of SCIENCE TEACHER in TX. The prevailing wage was 251052.0 month and the paid wage was 43800.0 year. The application was denied on 2014-04-07.\n",
      "4              On 2014-09-16, SIGNAL SCIENCES CORPORATION applied for a E-3 Australian visa for the position of SENIOR SOFTWARE ENGINEER in OR. The prevailing wage was 84573.0 bi-weekly and the paid wage was 170000.0 year. The application was denied on 2014-09-23.\n",
      "Name: Sentence, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Set display options to show the full width of the text\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "# Now display the first few rows of the 'Sentence' column\n",
    "print(df_cleaned['Sentence'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7f04af01-5c5a-4101-afea-c7cc965c59fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentences successfully saved to /Users/audrey/H-1B Insights AI/Sentences_H1B_Visa_Data.txt\n"
     ]
    }
   ],
   "source": [
    "# Save the sentences to a text file\n",
    "sentence_file_path = \"/Users/audrey/H-1B Insights AI/Sentences_H1B_Visa_Data.txt\"\n",
    "df_cleaned['Sentence'].to_csv(sentence_file_path, index=False, header=False)\n",
    "\n",
    "# Print a success message\n",
    "print(f\"Sentences successfully saved to {sentence_file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a8c123-b778-4664-8683-5aaba09ebd3a",
   "metadata": {},
   "source": [
    "# Train GPT-2 Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "29bf5dd0-3429-47b0-a421-d4cd3535afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['On 2014-03-14, ADVANCED TECHNOLOGY GROUP USA, INC. applied for a H-1B visa for the position of SOFTWARE ENGINEER in IL. The prevailing wage was 6217100.0 year and the paid wage was 62171.0 year. The application was denied on 2014-03-21.', 'On 2015-03-19, SAN FRANCISCO STATE UNIVERSITY applied for a greencard visa for the position of Assistant Professor of Marketing in CALIFORNIA. The prevailing wage was 5067600.0 year and the paid wage was 91440.0 year. The application was denied on 2015-03-19.', 'On 2013-09-13, CAROUSEL SCHOOL applied for a H-1B visa for the position of SPECIAL EDUCATION TEACHER in CA. The prevailing wage was 4947000.0 year and the paid wage was 49470.0 year. The application was denied on 2013-09-23.', 'On 2014-03-28, HARLINGEN CONSOLIDATED INDEPENDENT SCHOOL DISTRICT applied for a H-1B visa for the position of SCIENCE TEACHER in TX. The prevailing wage was 251052.0 month and the paid wage was 43800.0 year. The application was denied on 2014-04-07.', 'On 2014-09-16, SIGNAL SCIENCES CORPORATION applied for a E-3 Australian visa for the position of SENIOR SOFTWARE ENGINEER in OR. The prevailing wage was 84573.0 bi-weekly and the paid wage was 170000.0 year. The application was denied on 2014-09-23.']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the sentence data\n",
    "sentences_file = \"/Users/audrey/H-1B Insights AI/Sentences_H1B_Visa_Data.txt\"\n",
    "sentences = pd.read_csv(sentences_file, header=None)[0].tolist()\n",
    "\n",
    "# Check the first few sentences\n",
    "print(sentences[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0bb21770-cafb-4bfe-baf5-4485f9f9833d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2202,  1946,    12,  ..., 50256, 50256, 50256],\n",
      "        [ 2202,  1853,    12,  ..., 50256, 50256, 50256],\n",
      "        [ 2202,  2211,    12,  ..., 50256, 50256, 50256],\n",
      "        ...,\n",
      "        [ 2202,  2813,    12,  ..., 50256, 50256, 50256],\n",
      "        [ 2202,  2813,    12,  ..., 50256, 50256, 50256],\n",
      "        [ 2202,  2813,    12,  ..., 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer\n",
    "\n",
    "# Load the pre-trained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Set the pad token to the eos token (end-of-sequence token)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the sentences\n",
    "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=128)\n",
    "\n",
    "# Check the tokenized inputs\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a3bd5a69-4e0e-42c8-8e61-488ebcdda4aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[ 2202,  1946,    12,  3070,    12,  1415,    11, 43685, 20940,  1961,\n",
      "         44999,    45, 43781, 44441,  4916,    11, 19387,    13,  5625,   329,\n",
      "           257,   367,    12,    16,    33, 14552,   329,   262,  2292,   286,\n",
      "         47466, 36924,  8881,  1137,   287, 14639,    13,   383, 26602,  7699,\n",
      "           373,  8190,  1558,  3064,    13,    15,   614,   290,   262,  3432,\n",
      "          7699,   373,  8190, 27192,    13,    15,   614,    13,   383,  3586,\n",
      "           373,  6699,   319,  1946,    12,  3070,    12,  2481,    13, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2202,  1853,    12,  3070,    12,  1129,    11, 37376,  8782, 20940,\n",
      "          1797,  8220, 35454, 49677,  9050,  5625,   329,   257,  4077,  9517,\n",
      "         14552,   329,   262,  2292,   286, 15286,  8129,   286, 22137,   287,\n",
      "         33290,  5064, 30649,  3539,    13,   383, 26602,  7699,   373,  2026,\n",
      "          3134,  8054,    13,    15,   614,   290,   262,  3432,  7699,   373,\n",
      "           860,  1415,  1821,    13,    15,   614,    13,   383,  3586,   373,\n",
      "          6699,   319,  1853,    12,  3070,    12,  1129,    13, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n",
      "{'input_ids': tensor([[ 2202,  2211,    12,  2713,    12,  3312,    11, 20634,  5673,    42,\n",
      "            11, 19387,    13,  5625,   329,   257,   367,    12,    16,    33,\n",
      "         14552,   329,   262,  2292,   286, 47466, 36924,  8881,  1137,   287,\n",
      "         18732,    13,   383, 26602,  7699,   373, 14436, 29334,    13,    15,\n",
      "          1227,   290,   262,  3432,  7699,   373, 14436, 29334,    13,    15,\n",
      "           614,    13,   383,  3586,   373,  6699,   319,  2211,    12,  2713,\n",
      "            12,  2919,    13, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2202,  1946,    12,  3070,    12,  2078,    11, 43638,    43,  2751,\n",
      "          1677, 39537,  3535,  2389, 11617,  3268, 46162, 10619,  3525, 35883,\n",
      "         45057,  5625,   329,   257,   367,    12,    16,    33, 14552,   329,\n",
      "           262,  2292,   286,  6374, 42589, 13368, 16219,  1137,   287, 15326,\n",
      "            13,   383, 26602,  7699,   373,  1679,   940,  4309,    13,    15,\n",
      "          1227,   290,   262,  3432,  7699,   373,   604,  2548,   405,    13,\n",
      "            15,   614,    13,   383,  3586,   373,  6699,   319,  1946,    12,\n",
      "          3023,    12,  2998,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n",
      "{'input_ids': tensor([[ 2202,  2211,    12,  2713,    12,   940,    11,   402,  6684,    38,\n",
      "          2538, 19387,    13,  5625,   329,   257,   367,    12,    16,    33,\n",
      "         14552,   329,   262,  2292,   286, 47466, 36924,  8881,  1137,   287,\n",
      "          6645,    13,   383, 26602,  7699,   373,  1105,  1065, 21601,    13,\n",
      "            15,   614,   290,   262,  3432,  7699,   373,  1467,  2388,    13,\n",
      "            15,   614,    13,   383,  3586,   373,  6699,   319,  2211,    12,\n",
      "          2713,    12,  1415,    13, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2202,  1946,    12,  3070,    12,   940,    11, 37269, 15490, 39537,\n",
      "         16724,  2751,    11, 19387,    13,  5625,   329,   257,   367,    12,\n",
      "            16,    33, 14552,   329,   262,  2292,   286, 47466, 36924,  8881,\n",
      "          1137,   287,  7257,    13,   383, 26602,  7699,   373,   860,  4521,\n",
      "          2425,    13,    15,  1227,   290,   262,  3432,  7699,   373,   860,\n",
      "          4521,  2425,    13,    15,   614,    13,   383,  3586,   373,  6699,\n",
      "           319,  1946,    12,  3070,    12,  1507,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n",
      "{'input_ids': tensor([[ 2202,  2211,    12,  3070,    12,  1495,    11,   350, 11335, 46366,\n",
      "         11879,    11, 19387,    13,  5625,   329,   257,   367,    12,    16,\n",
      "            33, 14552,   329,   262,  2292,   286, 44738, 41254, 47466, 36924,\n",
      "          8881,  1137,   287,  7257,    13,   383, 26602,  7699,   373,  1511,\n",
      "         20370,  2078,    13,    15,   614,   290,   262,  3432,  7699,   373,\n",
      "         20299,   830,    13,    15,   614,    13,   383,  3586,   373,  6699,\n",
      "           319,  2211,    12,  3070,    12,  2078,    13, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2202,  2211,    12,  2931,    12,  1485,    11, 17368, 20958,  3698,\n",
      "         35883,  5625,   329,   257,   367,    12,    16,    33, 14552,   329,\n",
      "           262,  2292,   286, 38846,  8392,  9598,  6234, 13368, 16219,  1137,\n",
      "           287,  7257,    13,   383, 26602,  7699,   373,  5125,  2857,   830,\n",
      "            13,    15,   614,   290,   262,  3432,  7699,   373,  5125, 27790,\n",
      "            13,    15,   614,    13,   383,  3586,   373,  6699,   319,  2211,\n",
      "            12,  2931,    12,  1954,    13, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n",
      "{'input_ids': tensor([[ 2202,  1946,    12,  3023,    12,  2999,    11, 20176,    38,  3620,\n",
      "          1268,    40,   471,    13,    50,    13, 11419,  5625,   329,   257,\n",
      "           367,    12,    16,    33, 14552,   329,   262,  2292,   286,  6375,\n",
      "          2246,  2538,  6374,    44,  3537,  1847,    56,  2257,    14, 45346,\n",
      "         44180,  3537,  1847,    56,  2257,   287,  7257,    13,   383, 26602,\n",
      "          7699,   373,  1367,  2623,   940,    13,    15,  1227,   290,   262,\n",
      "          3432,  7699,   373,  1367,  2598,  2481,    13,    15,   614,    13,\n",
      "           383,  3586,   373,  6699,   319,  1946,    12,  3023,    12,  2931,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256],\n",
      "        [ 2202,  1946,    12,  2931,    12,  1433,    11, 36771,  1847,  6374,\n",
      "            40, 24181,  1546, 23929, 44680,  6234,  5625,   329,   257,   412,\n",
      "            12,    18,  6638, 14552,   329,   262,  2292,   286, 44738, 41254,\n",
      "         47466, 36924,  8881,  1137,   287,  6375,    13,   383, 26602,  7699,\n",
      "           373,   807,  2231,  4790,    13,    15,  3182,    12, 45291,   290,\n",
      "           262,  3432,  7699,   373,  1596,  2388,    13,    15,   614,    13,\n",
      "           383,  3586,   373,  6699,   319,  1946,    12,  2931,    12,  1954,\n",
      "            13, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256,\n",
      "         50256, 50256, 50256, 50256, 50256, 50256, 50256, 50256]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0]])}\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, inputs):\n",
    "        self.input_ids = inputs['input_ids']\n",
    "        self.attention_mask = inputs['attention_mask']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx]\n",
    "        }\n",
    "\n",
    "# Create a dataset\n",
    "train_dataset = TextDataset(inputs)\n",
    "\n",
    "# Use only a subset (first 10 examples) for faster demonstration\n",
    "subset_indices = list(range(10))  # First 10 indices\n",
    "train_subset = Subset(train_dataset, subset_indices)\n",
    "\n",
    "# Create a DataLoader with the subset\n",
    "train_loader = DataLoader(train_subset, batch_size=2, shuffle=True)  # Adjust batch_size as needed\n",
    "\n",
    "# Iterate over the DataLoader\n",
    "for batch in train_loader:\n",
    "    print(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d190ec84-a834-4b15-886c-bd9e9a0f3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "Total loss: 17.3248\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "# Load the pre-trained GPT-2 model\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Set the model to training mode\n",
    "model.train()\n",
    "\n",
    "epochs = 1  # Reduced for faster training\n",
    "for epoch in range(epochs):\n",
    "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Move batch to device (GPU/CPU)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "\n",
    "        # Get model output and compute loss\n",
    "        outputs = model(input_ids, attention_mask=attention_mask, labels=input_ids)\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Backpropagate the loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Total loss: {total_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4e98a-d308-4d9a-b6fa-62db6435afe0",
   "metadata": {},
   "source": [
    "### Note: This example runs for only 1 epoch for demonstration purposes.\n",
    "### The total loss (e.g., 17.3248) is high because the model has only been trained for a single epoch.\n",
    "### In a real-world application, you'd typically run for multiple epochs to allow the model to learn from the data.\n",
    "### With more epochs and tuning, the loss would gradually decrease, leading to better model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a436c687-5384-4d3e-a28d-e151ef123348",
   "metadata": {},
   "source": [
    "# Save trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "577eed1d-d0e9-4354-b335-34e6aee17b9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./fine_tuned_gpt2/tokenizer_config.json',\n",
       " './fine_tuned_gpt2/special_tokens_map.json',\n",
       " './fine_tuned_gpt2/vocab.json',\n",
       " './fine_tuned_gpt2/merges.txt',\n",
       " './fine_tuned_gpt2/added_tokens.json')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer.save_pretrained('./fine_tuned_gpt2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e84429-789c-452c-bd2f-5fd5fcdd6b8c",
   "metadata": {},
   "source": [
    "# Test Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "cc053fd9-3fdc-486a-846c-c68b927da5ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Can you tell me about the H-1B visa process?\n",
      "Generated Response: Can you tell me about the H-1B visa process?\n",
      "A: The U.S.-based company that is responsible for processing applications from foreign nationals, has been working with us to ensure we are able and willing to provide a fair opportunity in order to meet our needs as well as providing an affordable solution which will allow them to stay here permanently without having their visas revoked or denied by any government agency at all!.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "import torch\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = GPT2LMHeadModel.from_pretrained('./fine_tuned_gpt2')\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('./fine_tuned_gpt2')\n",
    "\n",
    "# Set the model to evaluation mode (not training)\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU available\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "\n",
    "# Example function to generate text\n",
    "def generate_text(prompt):\n",
    "    # Tokenize the prompt and prepare it for the model\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    # Generate output with additional settings for better text control\n",
    "    outputs = model.generate(\n",
    "        **inputs,\n",
    "        max_length=100,                  # Limit the length to avoid overly long outputs\n",
    "        num_return_sequences=1,          # Generate only one response\n",
    "        pad_token_id=tokenizer.eos_token_id,  # Set pad token to the end of sequence token\n",
    "        top_k=50,                        # Top-k sampling for diversity\n",
    "        top_p=0.95,                      # Nucleus sampling\n",
    "        temperature=0.7,                 # Controls randomness: lower values mean more deterministic\n",
    "        repetition_penalty=1.2           # Penalize repetitions to reduce redundant output\n",
    "    )\n",
    "    \n",
    "    # Decode the generated tokens back into text\n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean up the text to make it look better (remove redundant sentences)\n",
    "    cleaned_text = post_process_output(generated_text)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Post-processing function to handle simple cleaning of the output\n",
    "def post_process_output(text):\n",
    "    # Remove repeated sentences by splitting and rejoining sentences\n",
    "    sentences = text.split('. ')\n",
    "    cleaned_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence not in cleaned_sentences:\n",
    "            cleaned_sentences.append(sentence)\n",
    "    \n",
    "    return '. '.join(cleaned_sentences) + '.' if len(cleaned_sentences) > 0 else text\n",
    "\n",
    "# Test the model with a prompt\n",
    "prompt = \"Can you tell me about the H-1B visa process?\"\n",
    "generated_text = generate_text(prompt)\n",
    "\n",
    "# Print out the generated text\n",
    "print(\"Prompt:\", prompt)\n",
    "print(\"Generated Response:\", generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2beab7e1-48e5-43fd-aa45-ea07349bdd94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
